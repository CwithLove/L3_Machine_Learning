{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet: Modèles linéaires:  Adaline et Regression Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons nous intéresser à l'implémentation d'un algorithme de descente de gradient pour trouver le meilleur paramètre d'un module Adaline ou de regression logistique.\n",
    "\n",
    "Pout cela, on implémentera un algorithme de descente de gradient stochastique que nous avons vu au TP précédent et dont le pseudo-code peut être résumé comme suit:\n",
    "\n",
    "```input: Train, eta, m, MaxEp, modele\n",
    "init : w\n",
    "epoque=0\n",
    "while epoque<=MaxEp\n",
    "    choisir un exemple (x,y) de Train de façon aléatoire\n",
    "    calculer h = w*x\n",
    "    calculer Loss(h, y)\n",
    "    w <- w - eta*\"gradient de Loss(h, y) par rapport à w\"\n",
    "    epoque <- epoque+1\n",
    "output: w\n",
    "```\n",
    "où \"eta\" est le pas de la descente de gradient (exemple: eta=0.01).\n",
    "\n",
    "Si on veut imprimer l'erreur tous les \"m\" pas de gradient:\n",
    "```input: Train, eta, m, MaxEp, modele\n",
    "init : w\n",
    "epoque=0\n",
    "while epoque<=MaxEp\n",
    "    err = 0\n",
    "    for i in range(m):\n",
    "        choisir un exemple (x,y) de Train de façon aléatoire\n",
    "        calculer h = w*x\n",
    "        err += Loss(h, y)\n",
    "        w <- w - eta*\"gradient de Loss(h, y) par rapport à w\"\n",
    "    epoque <- epoque+1\n",
    "    print(err)\n",
    "output: w\n",
    "```\n",
    "\n",
    "Pour un poids $w$, on définit $h_\\mathbf{w}(\\mathbf{x})=w_0x_0+w_1x_1+...w_dx_d$. Pour chacun des deux modèles, et pour un exemple $(\\mathbf{x},y)$, la prédiction $\\hat{y}(\\mathbf{w}, \\mathbf{x})$ et la fonction de coût  $\\mathcal{L}(\\mathbf{w}, \\mathbf{x})$ sont: \n",
    "- Adaline: $\\hat{y}(\\mathbf{w}, \\mathbf{x}) = h_\\mathbf{w}(x)$ et $$\\mathcal{L}(\\mathbf{w})=(y-\\hat{y}(\\mathbf{w},\\mathbf{x}))^2=(y-h_\\mathbf{w}(\\mathbf{x}))^2,$$\n",
    "- Régression logistique: $\\hat{y}(w, x) = 1/(1+e^{-h_{\\mathbf{w}}(\\mathbf{x})})$ et $$\\mathcal{L}(\\mathbf{w}, x) = - y \\log \\hat{y}(\\mathbf{w},\\mathbf{x}) - (1-y)\\log(1-\\hat{y}(\\mathbf{w},\\mathbf{x})) = \\log(1+e^{h_{\\mathbf{w}}(\\mathbf{x})})-yh_\\mathbf{w}(\\mathbf{x}),$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Codage des fontions de perte et de gradient #\n",
    "###############################################\n",
    "\n",
    "# Adaline #\n",
    "def h(w,x): # Prediction\n",
    "    \"\"\"\n",
    "    w : vecteur de poids\n",
    "    x : vecteur d'entrée\n",
    "    Retourne le produit scalaire de w et x\n",
    "    \"\"\"\n",
    "    return w@x\n",
    "\n",
    "def loss_adaline(p,y):\n",
    "    \"\"\"\n",
    "    p : prédiction\n",
    "    y : valeur réelle\n",
    "    Retourne la perte quadratique entre p et y\n",
    "    \"\"\"\n",
    "    return (y-p)**2\n",
    "\n",
    "def adaline_grad(w,x,y):\n",
    "    \"\"\"\n",
    "    w : vecteur de poids\n",
    "    x : vecteur d'entrée\n",
    "    y : valeur réelle\n",
    "    Retourne le gradient de la perte quadratique entre h(w,x) et y\n",
    "    \"\"\"\n",
    "    return 2*(h(w,x) - y)*x\n",
    "\n",
    "\n",
    "\n",
    "# Regression logistique #\n",
    "def prediction_regression_logistique(w,x):\n",
    "    \"\"\"\n",
    "    w : vecteur de poids\n",
    "    x : vecteur d'entrée\n",
    "    Retourne la prédiction de la régression logistique\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-h(w,x)))\n",
    "\n",
    "def loss_regression_logistique(p,y):\n",
    "    \"\"\"\n",
    "    p : prédiction\n",
    "    y : valeur réelle\n",
    "    Retourne la perte de la régression logistique\n",
    "    \"\"\"\n",
    "    p = np.clip(p, 1e-10, 1-1e-10) # Pour éviter les erreurs de calcul\n",
    "    return -y*np.log(p) - (1-y)*np.log(1-p)\n",
    "\n",
    "def regression_logistique_grad(w,x,y):\n",
    "    \"\"\"\n",
    "    w : vecteur de poids\n",
    "    x : vecteur d'entrée\n",
    "    y : valeur réelle\n",
    "    Retourne le gradient de la perte de la régression logistique\n",
    "    \"\"\"\n",
    "    return (prediction_regression_logistique(w,x)-y)*x\n",
    "\n",
    "# Gradient\n",
    "def gradient(grad, w, x, y):\n",
    "    \"\"\"\n",
    "    Retourne le gradient de la fonction de perte\n",
    "    \"\"\"\n",
    "    return grad(w,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu les gradients de ces fonctions en TD.\n",
    "\n",
    "## Partie 1: implémentation de l'algorithme et exemple du \"ET logique\"\n",
    "\n",
    "<font color='red'><b>Question 1:</b> le \"ET logique\".</font> Créer une liste de 4 éléments où chaque élément est un couple de la forme `[x,y]`, avec `x=[1,x1,x2]` et `y = x1 and x2`. Il y a 4 éléments car `x1` et `x2` peuvent chacun prendre la valeur `0` ou `1` (chacun de ces 4 éléments est une liste dont le premier élément est les attributs de l'exemple et le deuxième élément est la classe de l'exemple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 0],\n",
       "        [1, 1, 0],\n",
       "        [1, 0, 1],\n",
       "        [1, 1, 1]]),\n",
       " array([0, 0, 0, 1]))"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "# Creation de liste ET logique #\n",
    "################################\n",
    "\n",
    "liste = [0] * 4\n",
    "for x1 in range(2):\n",
    "    for x2 in range(2):\n",
    "        liste[x1*1+x2*2] = [[1,x1,x2], x1 and x2] \n",
    "\n",
    "x_ET_logique = np.array([ei[0] for ei in liste])\n",
    "y_ET_logique = np.array([ei[1] for ei in liste])\n",
    "\n",
    "x_ET_logique, y_ET_logique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Question 2:</b></font> Coder un algorithme de descente de gradient stochastique pour les modèles Adaline et le modèle de régression logistique et le faire tourner sur le modèle de \"ET logique\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights for Adaline on 'ET logique': [0.03685315 0.21740134 0.20505566]\n",
      "Final weights for Logistic Regression on 'ET logique': [-1.84737059  1.00289146  1.00848033]\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Fonction Gradient Descent #\n",
    "#############################\n",
    "\n",
    "import random as rd\n",
    "    \n",
    "# Stochastic Gradient Descent #\n",
    "def SGD(X, Y, Maxep, eta, p, loss_function, g, err_printing=True):\n",
    "    \"\"\"\n",
    "    Descente de gradient stochastique\n",
    "    \n",
    "    Entrees:\n",
    "    X : tableau des features\n",
    "    Y : tableau des labels / target variable\n",
    "    Maxep : nombre d'epoques\n",
    "    eta : fonction de taux d'apprentissage (learning rate)\n",
    "    p : fonction de prediction\n",
    "    loss_function : fonction de perte\n",
    "    g : fonction de gradient\n",
    "    err_printing : Boolean affichage des erreurs (True par defaut)\n",
    "    \n",
    "    Sorties:\n",
    "    w : vecteur des poids\n",
    "    \"\"\"\n",
    "    w = np.random.rand(len(X[0])) * 0.01\n",
    "    for epoque in range(Maxep):\n",
    "        i = rd.randint(0, len(Y) - 1)\n",
    "        pred = p(w, X[i])\n",
    "        if err_printing:\n",
    "            print(loss_function(pred, Y[i]))\n",
    "        w = w - eta(epoque) * g(w, X[i], Y[i])\n",
    "    return w\n",
    "\n",
    "def SGD_with_error_printing(X, Y, Maxep, eta, m, p, loss_function, g, err_printing=True):\n",
    "    \"\"\"\n",
    "    Pareil que SGD mais la mise à jour des poids est faite m fois par époque\n",
    "    L'erreur correspond à la somme sur les m exemples traités durant l'époque\n",
    "    \"\"\"\n",
    "    w = np.random.rand(len(X[0])) # * 0.01\n",
    "    for epoque in range(Maxep):\n",
    "        err = 0\n",
    "        for _ in range(m):\n",
    "            i = rd.randint(0, len(Y) - 1)\n",
    "            pred = p(w, X[i])\n",
    "            err += loss_function(pred, Y[i])\n",
    "            w = w - eta(epoque) * g(w, X[i], Y[i])\n",
    "        if err_printing:\n",
    "            print(\"epoque \", epoque, \": \", err)\n",
    "    return w\n",
    "\n",
    "# Learning rate #   \n",
    "def eta_001(t):\n",
    "    return 0.01\n",
    "\n",
    "def eta_01(t):\n",
    "    return 0.1\n",
    "#################\n",
    "\n",
    "\n",
    "# Training Adaline on \"ET logique\"\n",
    "w_adaline = SGD(x_ET_logique, y_ET_logique, 100, eta_001, h, loss_adaline, adaline_grad, False)\n",
    "print(\"Final weights for Adaline on 'ET logique':\", w_adaline)\n",
    "\n",
    "# Training Logistic Regression on \"ET logique\"\n",
    "w_logistic = SGD_with_error_printing(x_ET_logique, y_ET_logique, 100, eta_01, 2, prediction_regression_logistique, loss_regression_logistique, regression_logistique_grad, False)\n",
    "print(\"Final weights for Logistic Regression on 'ET logique':\", w_logistic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer le taux d'erreur de votre algorithme sur cette base (où une erreur est comptabilisé si la prédiction est plus proche de la fausse classe que de la vraie classe). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Calculer le taux d'erreur #\n",
    "#############################\n",
    "\n",
    "def calculate_error_rate(w, X, Y, prediction_function, rate_printing=False):\n",
    "    \"\"\"\n",
    "    Calculer le taux d'erreur\n",
    "    \n",
    "    Entrees:\n",
    "    w : vecteur des poids\n",
    "    X : tableau des features\n",
    "    Y : tableau des labels / target variable\n",
    "    prediction_function : fonction de prediction\n",
    "    rate_printing : Boolean affichage le taux d'erreur (False par defaut)\n",
    "    \n",
    "    Sorties:\n",
    "    errors : taux d'erreur\n",
    "    \"\"\"\n",
    "    errors = 0\n",
    "    for i in range(len(Y)):\n",
    "        pred = prediction_function(w, X[i])\n",
    "        if rate_printing:\n",
    "            print(f\"P: {pred}, Y: {Y[i]}\")\n",
    "        if (pred >= 0.5 and Y[i] == 0) or (pred < 0.5 and Y[i] == 1):\n",
    "            errors += 1\n",
    "    return errors / len(Y)\n",
    "\n",
    "\n",
    "# Function to calculate error rate multiple times\n",
    "def calculate_multiple_error_rates(X, Y, num_iterations, model, prediction_function, loss, grad, eta, Maxep):\n",
    "    \"\"\"\n",
    "    Calculer le taux d'erreur plusieurs fois\n",
    "    \n",
    "    Objectif:\n",
    "    - Entrainer le modèle plusieurs fois\n",
    "    - Calculer le taux d'erreur à chaque itération\n",
    "    - Verifier la stabilité et la convergence de l'algorithme\n",
    "    \"\"\"\n",
    "    if model == 'adaline':\n",
    "        print(\"Training Adaline...\") \n",
    "    elif model == 'logistic':\n",
    "        print(\"Training Logistic Regression...\")\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        if model == 'adaline':\n",
    "            w = SGD(X, Y, Maxep, eta, prediction_function, loss, grad, False)\n",
    "        elif model == 'logistic':\n",
    "            w = SGD(X, Y, Maxep, eta, prediction_function, loss, grad, False)\n",
    "        error_rate = calculate_error_rate(w, X, Y, prediction_function, False)\n",
    "        print(\"Error rate:\", error_rate)\n",
    "\n",
    "\n",
    "# Calculate error rates for Adaline and Logistic Regression\n",
    "num_iterations = 25\n",
    "\n",
    "#####################################\n",
    "# Calculate error rates for Adaline #\n",
    "# calculate_multiple_error_rates(x_ET_logique, y_ET_logique, num_iterations, 'adaline', h, loss_adaline, adaline_grad, eta_001, 100)\n",
    "# calculate_multiple_error_rates(x_ET_logique, y_ET_logique, num_iterations, 'adaline', h, loss_adaline, adaline_grad, eta_01, 100)\n",
    "\n",
    "#################################################\n",
    "# Calculate error rates for Logistic Regression #\n",
    "# calculate_multiple_error_rates(x_ET_logique, y_ET_logique, num_iterations, 'logistic', prediction_regression_logistique, loss_regression_logistique, regression_logistique_grad, eta_001, 100)\n",
    "# calculate_multiple_error_rates(x_ET_logique, y_ET_logique, num_iterations, 'logistic', prediction_regression_logistique, loss_regression_logistique, regression_logistique_grad, eta_01, 100)\n",
    "# calculate_multiple_error_rates(x_ET_logique, y_ET_logique, num_iterations, 'logistic', prediction_regression_logistique, loss_regression_logistique, regression_logistique_grad, eta_01, 200)\n",
    "# calculate_multiple_error_rates(x_ET_logique, y_ET_logique, num_iterations, 'logistic', prediction_regression_logistique, loss_regression_logistique, regression_logistique_grad, eta_01, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2: premiers tests avec une base de donnée réelle\n",
    "\n",
    "<font color='red'><b>Question 3:</b></font> Nous allons maintenant nous intéresser au comportement de ces modèles sur la base SONAR de la collection UCI (http://archive.ics.uci.edu/ml/index.php). Cette base contient 208 exemples en dimension 60 séparés par `,` et la dernière élément correspond à la classe de l'exemple.\n",
    "\n",
    "    1. Télécharger la collection avec la fonction read_table de la librairie pandas (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html). Les options nécessaires sont `sep=','` et `header=None`  \n",
    "    2. Créer une liste de listes correspondant à la collection; pour cela initialiser la première liste et en parcourant chaque ligne de la matrice de données; créer une liste associée en remplaçant le dernier élément par `0` ou `+1` et insérer la dans la première liste. \n",
    "    Indication: Utiliser la fonction `loc`. \n",
    "    3. Écrire une fonction qui génère deux listes de données `x_train` (75%) and `x_test` (25%) en la mélangeant aléatoirement au préalable (indication: on pourra utiliser les fonctions `shuffle` de la librairie `random` et `train_test_split` de la librairie `sklearn.model_selection`)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "         58      59  60  \n",
       "0    0.0090  0.0032   R  \n",
       "1    0.0052  0.0044   R  \n",
       "2    0.0095  0.0078   R  \n",
       "3    0.0040  0.0117   R  \n",
       "4    0.0107  0.0094   R  \n",
       "..      ...     ...  ..  \n",
       "203  0.0193  0.0157   M  \n",
       "204  0.0062  0.0067   M  \n",
       "205  0.0077  0.0031   M  \n",
       "206  0.0036  0.0048   M  \n",
       "207  0.0061  0.0115   M  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sonar dataset #\n",
    "sonar_data_fram = pd.read_table('Data/Sonar/sonar.all-data', sep = ',', header = None)\n",
    "sonar_data_fram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fonction qui produit les listes de features et la liste de label\n",
    "- Fonction qui génère deux listes de données `x_train` (75%) and `x_test` (25%) en la mélangeant aléatoirement au préalable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_collection(dataFrame, numL, numC, targetC, classType=None):\n",
    "    \"\"\"\n",
    "    Entree:\n",
    "    dataFrame : le dataframe\n",
    "    numL : nombre de lignes\n",
    "    numC : nombre de colonnes\n",
    "    targetC : colonne label\n",
    "    classType : liste des classes (2 classes dans ce cas)\n",
    "    \n",
    "    Sortie:\n",
    "    x : tableau des features\n",
    "    y : tableau des labels \n",
    "    \"\"\"\n",
    "    x = np.zeros((numL, numC - 1))\n",
    "    y = np.zeros(numL)\n",
    "    \n",
    "    feature_idx = 0  # indice pour les colonnes de x\n",
    "    for i in range(numC):\n",
    "        if i == targetC:\n",
    "            continue  # on saute la colonne label\n",
    "        x[:, feature_idx] = dataFrame[i]\n",
    "        feature_idx += 1\n",
    "    \n",
    "    if classType == None:\n",
    "        for i in range(numL):\n",
    "            y[i] = dataFrame[targetC][i]\n",
    "    else:\n",
    "        for i in range(numL):\n",
    "            if dataFrame[targetC][i] == classType[0]: y[i] = 1\n",
    "            elif dataFrame[targetC][i] == classType[1]: y[i] = 0\n",
    "            else: \n",
    "                print(\"Erreur: classType doit contenir 2 classes\")\n",
    "                exit(0)\n",
    "            \n",
    "    return x, y\n",
    "\n",
    "def generate_train_test_random(x, y):\n",
    "    \"\"\"\n",
    "    Entree:\n",
    "    x : tableau des features\n",
    "    y : tableau des labels\n",
    "    \n",
    "    Sortie:\n",
    "    x_train : tableau des features pour l'entrainement\n",
    "    x_test : tableau des features pour le test\n",
    "    y_train : tableau des labels pour l'entrainement\n",
    "    y_test : tableau des labels pour le test\n",
    "    \"\"\" \n",
    "    data = list(zip(x, y))\n",
    "    rd.shuffle(data)\n",
    "    new_X = np.array([i for i, j in data])\n",
    "    new_Y = np.array([j for i, j in data])\n",
    "    x_train, x_test, y_train, y_test = train_test_split(new_X, new_Y, test_size=0.25)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Question 4:</b></font> Appliquer ces modèles sur cette base (on pourra prendre $MaxEp\\approx1000$ et le pas d'apprentissage $\\eta\\approx0.01$) et en choisissant les bases Train et Test de façon aléatoire \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_error(collection, targetC, classType, num_iterations, MaxEp, eta, err_iter=False):\n",
    "    x_collection, y_collection = produce_collection(collection, collection.shape[0], collection.shape[1], targetC, classType)\n",
    "    x_train, x_test, y_train, y_test = generate_train_test_random(x_collection, y_collection)\n",
    "    out = \"\"\n",
    "    ada = np.zeros(num_iterations)\n",
    "    rel = np.zeros(num_iterations)\n",
    "    for _ in range(num_iterations):\n",
    "        out += \"iter \" + str(_+1)  + \":\\n\"\n",
    "        out += \"Model Adaline: \"\n",
    "        w_adaline = SGD(x_train, y_train, MaxEp, eta, h, loss_adaline, adaline_grad, False)\n",
    "        error_rate = []\n",
    "        for x_i, y_i in zip(x_test, y_test):\n",
    "            error_rate.append(loss_adaline(h(w_adaline, x_i), y_i))\n",
    "        out += str(np.mean(error_rate)) + \"\\n\"\n",
    "        ada[_] = np.mean(error_rate)\n",
    "        \n",
    "        out += \"Model Logistic Regression: \"\n",
    "        w_logistic = SGD(x_train, y_train, MaxEp, eta, prediction_regression_logistique, loss_regression_logistique, regression_logistique_grad, False)\n",
    "        error_rate = []\n",
    "        for x_i, y_i in zip(x_test, y_test):\n",
    "            error_rate.append(loss_regression_logistique(prediction_regression_logistique(w_logistic, x_i), y_i))\n",
    "        out += str(np.mean(error_rate)) + \"\\n\"\n",
    "        rel[_] = np.mean(error_rate)\n",
    "    if err_iter:\n",
    "        print(out)\n",
    "    \n",
    "    print(np.mean(ada), np.mean(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reporter l'erreur moyenne de ces modèles obtenues sur les exemples de donnés de \"test\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1:\n",
      "Model Adaline: 0.18112945411483233\n",
      "Model Logistic Regression: 0.6349223733147699\n",
      "\n",
      "0.18112945411483233 0.6349223733147699\n"
     ]
    }
   ],
   "source": [
    "class_type_sonar = ['M', 'R']\n",
    "MaxEp = 1000\n",
    "calculate_mean_error(sonar_data_fram, sonar_data_fram.shape[1] - 1, class_type_sonar, 1, MaxEp, eta_001, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Refaire l'opération 3 fois avec trois randomisations différentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1:\n",
      "Model Adaline: 0.14834995501534148\n",
      "Model Logistic Regression: 0.6127587041779408\n",
      "iter 2:\n",
      "Model Adaline: 0.15428029991102285\n",
      "Model Logistic Regression: 0.6318251859712103\n",
      "iter 3:\n",
      "Model Adaline: 0.12243317036184341\n",
      "Model Logistic Regression: 0.6112738214479116\n",
      "\n",
      "0.14168780842940257 0.6186192371990209\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_error(sonar_data_fram, sonar_data_fram.shape[1] - 1, class_type_sonar, 3, MaxEp, eta_001, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  | Collection | Adaline     | Régression Logistique |\n",
    "  |------------|-------------|-----------------------|\n",
    "  |   SONAR (réplica 1)   |            |           |\n",
    "  |   SONAR (réplica 2)   |           |         |\n",
    "  |   SONAR (réplica 3)   |           |           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3: normalisation\n",
    "\n",
    "Nous allons étudier l'impact de la nomralisation sur les prédictions. Pour cela nous considérons deux stratégies de normalisation communément utilisées dans la littérature:\n",
    "* Stratégie <i>max</i>: consiste à normaliser chaque caractéristique du vecteur réprésentatif d'une observation par la valeur maximale de cette caractéristiques\n",
    "* Stratégie <i>norme</i>: consiste à normaliser chaque caractéristique du vecteur réprésentatif d'une observation par la norme de ce vecteur.\n",
    "\n",
    "Nous considérons ces trois autres collections de la base UCI:\n",
    "\n",
    "        * https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
    "        * https://archive.ics.uci.edu/ml/datasets/spambase\n",
    "        * https://archive.ics.uci.edu/ml/datasets/ionosphere\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer dataset #\n",
    "breast_cancer_df = pd.read_table('Data/BreastCancer/wdbc.data', header=None, sep=',')\n",
    "ionosphere_df = pd.read_csv('Data/Ionosphere/ionosphere.data', header=None, sep=',')\n",
    "spambase_df = pd.read_csv('Data/Spambase/spambase.data', header=None, sep=',')\n",
    "\n",
    "# breast_cancer_df\n",
    "# ionosphere_df\n",
    "# spambase_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='red'><b>Question 5:</b></font> Ecrire une fonction qui prend en entrée la collection des données et qui retourne la collections normalisée suivant les stratégies <i>max</i> et <i>norme</i>. \n",
    "\n",
    "- Choisir tous les colonnes contenant des données numériques et les convertir en type `float` pour é viter les problème de division.\n",
    "-  <i>Max</i>: Diviser par la valeur maximale en valeur absolu\n",
    "-  <i>Norme</i>: Norme L2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer_collection(collection, strategies='max'):\n",
    "    normalized_collection = collection.copy()\n",
    "    \n",
    "    numeric_cols = normalized_collection.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    numeric_df = normalized_collection[numeric_cols].astype(float).copy()\n",
    "    \n",
    "    if strategies == 'max':\n",
    "        max_value = np.max(np.abs(numeric_df), axis=0)\n",
    "        max_value[max_value == 0] = 1 \n",
    "        numeric_df = numeric_df / max_value\n",
    "    \n",
    "    elif strategies == 'norme': \n",
    "        l2_norm = np.linalg.norm(numeric_df, ord=2, axis=0)\n",
    "        l2_norm[l2_norm == 0] = 1  \n",
    "        numeric_df = numeric_df / l2_norm\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid normalization strategy\")\n",
    "    \n",
    "    normalized_collection[numeric_cols] = numeric_df\n",
    "    return normalized_collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><b>Question 6:</b></font> Compléter les tableaux comparatifs suivants en repertant les erreurs moyennes sur 20 lancements des modèles de l'Adaline et de la Régression Logistique et pour les trois cas:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1      2      3       4       5        6        7        8   \\\n",
       "0      842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
       "1      842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
       "2    84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
       "3    84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
       "4    84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
       "..        ... ..    ...    ...     ...     ...      ...      ...      ...   \n",
       "564    926424  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
       "565    926682  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
       "566    926954  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
       "567    927241  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
       "568     92751  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
       "\n",
       "          9   ...      22     23      24      25       26       27      28  \\\n",
       "0    0.14710  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iterations = 20\n",
    "breast_cancer_df\n",
    "# calculate_mean_error(breast_cancer_df, 1, ['M', 'B'], num_iterations, MaxEp, eta_001)\n",
    "# calculate_mean_error(ionosphere_df, ionosphere_df.shape[1] - 1, ['g', 'b'], num_iterations, MaxEp, eta_001)\n",
    "# calculate_mean_error(sonar_data_fram, sonar_data_fram.shape[1] - 1, class_type_sonar, num_iterations, MaxEp, eta_001)\n",
    "# calculate_mean_error(spambase_df, spambase_df.shape[1] - 1, None, num_iterations, MaxEp, eta_001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " '*' Les vecteurs ne sont pas normalisés\n",
    "     \n",
    "  | Collection |   Adaline   |  Régression Logistique |\n",
    "  |------------|-------------|------------------------|\n",
    "  |   BREAST   |             |                        |\n",
    "  |   IONO     |             |                        |\n",
    "  |   SONAR    |             |                        |\n",
    "  |   SPAM     |             |                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer:\n",
      "0.200095044600436 0.6897413022564496\n",
      "Ionosphere:\n",
      "0.29565636171942816 0.6781567460900401\n",
      "Sonar:\n",
      "0.2345651841815568 0.7017322227684746\n",
      "Spambase:\n",
      "0.00019086982303134217 0.6913760446149266\n"
     ]
    }
   ],
   "source": [
    "# Normalisation avec strategie norme #\n",
    "breast_cancer_df_norme = normalizer_collection(breast_cancer_df, 'norme')\n",
    "ionosphere_df_norme = normalizer_collection(ionosphere_df, 'norme')\n",
    "sonar_data_fram_norme = normalizer_collection(sonar_data_fram, 'norme')\n",
    "spambase_df_norme = normalizer_collection(spambase_df, 'norme')\n",
    "\n",
    "print(\"Breast Cancer:\")\n",
    "calculate_mean_error(breast_cancer_df_norme, 1, ['M', 'B'], num_iterations, MaxEp, eta_001)\n",
    "print(\"Ionosphere:\")\n",
    "calculate_mean_error(ionosphere_df_norme, ionosphere_df_norme.shape[1] - 1, ['g', 'b'], num_iterations, 1000, eta_001)\n",
    "print(\"Sonar:\")\n",
    "calculate_mean_error(sonar_data_fram_norme, sonar_data_fram_norme.shape[1] - 1, class_type_sonar, num_iterations, 1000, eta_001)\n",
    "print(\"Spambase:\")\n",
    "calculate_mean_error(spambase_df_norme, spambase_df_norme.shape[1] - 1, None, num_iterations, 1000, eta_001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " $^n$ Normalisation suivant la stratégie <i>norme</i>\n",
    "     \n",
    "  | Collection |   Adaline   |  Régression Logistique |\n",
    "  |------------|-------------|------------------------|\n",
    "  |   BREAST   |             |                        |\n",
    "  |   IONO     |             |                        |\n",
    "  |   SONAR    |             |                        |\n",
    "  |   SPAM     |             |                        |\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer:\n",
      "0.08652205518087261 0.5373295590226499\n",
      "Ionosphere:\n",
      "0.19857414236777765 0.6327969576549477\n",
      "Sonar:\n",
      "0.20154379597658417 0.6742839942520126\n",
      "Spambase:\n",
      "0.2173167331479255 0.6807205917936906\n"
     ]
    }
   ],
   "source": [
    "# Normalisation avec strategie max #\n",
    "breast_cancer_df_max = normalizer_collection(breast_cancer_df)\n",
    "spambase_df_max = normalizer_collection(spambase_df)\n",
    "ionosphere_df_max = normalizer_collection(ionosphere_df)\n",
    "\n",
    "print(\"Breast Cancer:\")\n",
    "calculate_mean_error(breast_cancer_df_max, 1, ['M', 'B'], num_iterations, MaxEp, eta_001)\n",
    "print(\"Ionosphere:\")\n",
    "calculate_mean_error(ionosphere_df_max, ionosphere_df_max.shape[1] - 1, ['g', 'b'], num_iterations, MaxEp, eta_001)\n",
    "print(\"Sonar:\")\n",
    "calculate_mean_error(sonar_data_fram, sonar_data_fram.shape[1] - 1, class_type_sonar, num_iterations, MaxEp, eta_001)\n",
    "print(\"Spambase:\")\n",
    "calculate_mean_error(spambase_df_max, spambase_df_max.shape[1] - 1, None, num_iterations, MaxEp, eta_001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $^m$ Normalisation suivant la stratégie <i>max</i>\n",
    "    \n",
    "  | Collection |   Adaline   |  Régression Logistique |\n",
    "  |------------|-------------|------------------------|\n",
    "  |   BREAST   |             |                        |\n",
    "  |   IONO     |             |                        |\n",
    "  |   SONAR    |             |                        |\n",
    "  |   SPAM     |             |                        |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
